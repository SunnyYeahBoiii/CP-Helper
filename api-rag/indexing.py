from semantic_router.splitters import RollingWindowSplitter
from semantic_router.utils.logger import logger
from semantic_router.encoders import HuggingFaceEncoder
import numpy as np
from dotenv import load_dotenv
from pinecone import Pinecone, ServerlessSpec
import os

from unstructured.partition.md import partition_md

def process_mdx_advanced(file_path):
    # partition_md t·ª± ƒë·ªông x·ª≠ l√Ω frontmatter v√† c·∫•u tr√∫c markdown
    elements = partition_md(filename=file_path)
    
    # G·ªôp c√°c element l·∫°i th√†nh vƒÉn b·∫£n
    full_text = "\n\n".join([str(el) for el in elements])
    return full_text

load_dotenv()

encoder = HuggingFaceEncoder(name="nomic-ai/nomic-embed-text-v1.5", score_threshold=0.5 , trust_remote_code=True)

logger.setLevel("WARNING")

splitter = RollingWindowSplitter(
    encoder=encoder,
    min_split_tokens=50,
    max_split_tokens=20000,
    window_size=2,
    plot_splits=True,
    enable_statistics=True
)

pinecone_api = os.getenv("PINECONE_API_KEY")
pc = Pinecone(api_key=pinecone_api)

spec = ServerlessSpec(
    cloud='aws',  # or 'gcp', 'azure'
    region='us-east-1'  # choose appropriate region
)

# Create index if not exists
index_name = 'cp-rag'
if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=768,  # Match your encoder's output dim (e.g., BGE-M3 is 1024? Wait, check encoder)
        metric='cosine',
        spec=spec
    )

# Connect to index
index = pc.Index(index_name)

def build_chunk(title: str , content: str):
        return f"# {title} \n{content}"

def insert_embeddings(title:str , file_path: str):
    text = process_mdx_advanced(file_path=file_path)

    splits = splitter([text])

    metadata = []
    for i, s in enumerate(splits):
        pre_chunk = "" if i == 0 else splits[i - 1].content
        post_chunk = "" if i + 1 == len(splits) else splits[i + 1].content

        metadata.append({
            "title": title,
            "content": s.content,
            "prechunk": pre_chunk,
            "postchunk": post_chunk,
        })

    vectors = []
    for i, meta in enumerate(metadata):
        # Embed the content
        embedding = encoder([meta['content']])[0]  # or use splitter/encoder as needed
        vectors.append({
            'id': f'${title}-{i}',
            'values': embedding,  # Convert to list
            'metadata': {
                'title': meta['title'],
                'content': meta['content'],
                'prechunk': meta['prechunk'],
                'postchunk': meta['postchunk']
            }
        })

    print(vectors)
    # Upsert in batches
    batch_size = 100
    for i in range(0, len(vectors), batch_size):
        batch = vectors[i:i+batch_size]
        index.upsert(vectors=batch)
    
import glob
import re
from tqdm import tqdm 

def process_all_documents(input_folder, output_folder=None):
    # T·∫°o ƒë∆∞·ªùng d·∫´n t√¨m ki·∫øm: ./documents/*.mdx
    # recursive=True gi√∫p t√¨m c·∫£ trong c√°c th∆∞ m·ª•c con (n·∫øu c√≥)
    search_path = os.path.join(input_folder, "**", "*.mdx")
    
    # L·∫•y danh s√°ch t·∫•t c·∫£ file mdx
    files = glob.glob(search_path, recursive=True)
    
    print(f"üîç T√¨m th·∫•y {len(files)} file .mdx trong '{input_folder}'")
    
    results = []
    
    # D√πng tqdm ƒë·ªÉ hi·ªán thanh loading bar khi ch·∫°y
    for file_path in tqdm(files, desc="ƒêang x·ª≠ l√Ω"):
        print(file_path)
        try:
            title = file_path[12:]
            title = title[:-4]
            
            insert_embeddings(title=title , file_path=file_path)
            
        except Exception as e:
            print(f"‚ùå L·ªói khi ƒë·ªçc file {file_path}: {e}")
            
    return results

def file_listing():
    INPUT_DIR = "./documents"
    OUTPUT_DIR = "./processed_txt" # N∆°i l∆∞u file txt k·∫øt qu·∫£ (n·∫øu c·∫ßn)
    
    # C√†i ƒë·∫∑t tqdm n·∫øu ch∆∞a c√≥: pip install tqdm
    if not os.path.exists(INPUT_DIR):
        print(f"‚ö†Ô∏è Th∆∞ m·ª•c '{INPUT_DIR}' kh√¥ng t·ªìn t·∫°i!")
        # T·∫°o th∆∞ m·ª•c m·∫´u ƒë·ªÉ test
        os.makedirs(INPUT_DIR)
        print(f"‚úÖ ƒê√£ t·∫°o th∆∞ m·ª•c m·∫´u '{INPUT_DIR}'. H√£y copy file .mdx v√†o ƒë√≥.")
    else:
        # G·ªçi h√†m x·ª≠ l√Ω
        processed_data = process_all_documents(INPUT_DIR, OUTPUT_DIR)
        
        print(f"\n‚úÖ Ho√†n t·∫•t! ƒê√£ x·ª≠ l√Ω {len(processed_data)} file.")
        
        # V√≠ d·ª•: In th·ª≠ n·ªôi dung file ƒë·∫ßu ti√™n

file_listing()